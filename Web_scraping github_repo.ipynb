{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web-Scraping project on Github topics page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About Web-scraping\n",
    "Web scraping is an automated method used to extract large amounts of data from websites quickly and efficiently. This process involves fetching web pages and extracting relevant information, which can then be stored and analyzed for various purposes such as data analysis, research, and machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About Github (since we will scraping the topics page of github)\n",
    "Well, since this is already on github, you know about it. Putting it simply, it's a disneyland for developers where projects come to life."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project aims to extract and analyze data from GitHub repositories for various topics using web scraping techniques. By scraping data from GitHub, we can gain insights into the most popular repositories, trends in different fields, and the activity levels of various projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objectives\n",
    "1. Scrape GitHub Repositories: Extract data from GitHub repository pages for specific topics.\n",
    "2. Data Parsing and Storage: Parse the HTML content and store the extracted data in a structured format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Technologies used\n",
    "1. Python: All code is in python, since it is versatile\n",
    "2. Jupyter-Notebook: It is the platform used to write the code\n",
    "\n",
    "#### Libraries used\n",
    "1. Pandas: For data manipulation\n",
    "2. Requests: For sending HTTP requests\n",
    "3. OS: To interact with the operating system, since we are saving the data in the form of csv's on the device\n",
    "4. BeatifulSoup: Parsing HTML files and extract data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project workflow:\n",
    "1. Setting up the environment\n",
    "2. Web scraping\n",
    "3. Data parsing and storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Note: Currently, this only includes only scraping of the topics in the first page of main topics page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  1. Setting up the environment:\n",
    "Importing all the necessary libraries that will be used for the task of web-scraping data from the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Web Scraping:\n",
    "- First we will use the required url of the github topics page to create a secure connection to the page.\n",
    "- Then store all the data, relevant or irrelevant, into a variable using BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_url='https://github.com/topics'\n",
    "resp= requests.get(base_url)\n",
    "resp.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_contents=resp.text\n",
    "\n",
    "with open('webpage.html','w') as f:\n",
    "    f.write(page_contents)\n",
    "\n",
    "doc=BeautifulSoup(page_contents,'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We get the number of likes or star count, below function converts the string into integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_star_count(stars_str):\n",
    "    stars_str=stars_str.strip()\n",
    "    if stars_str[-1]=='k':\n",
    "        return int(float(stars_str[:-1])*1000)\n",
    "    return int(stars_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All data is converted into relevant text, using BeatifulSoup library, with the help of class name and tag type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repo_info(h3_tag,star_tag):\n",
    "    #returns all the required information about a particular repository\n",
    "    a_tags=h3_tag.find_all('a')\n",
    "    username=a_tags[0].text.strip()\n",
    "    repo_name=a_tags[1].text.strip()\n",
    "    repo_url=base_url+a_tags[1].text.strip()\n",
    "    stars_count=parse_star_count(star_tag.text.strip())\n",
    "    return username,repo_name,repo_url,stars_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After a secure connection, all repo info is stored in the form of a dataframe, below functions exactly these tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_page(topic_url):\n",
    "    #download the page\n",
    "    response=requests.get(topic_url)\n",
    "\n",
    "    #check response\n",
    "    if response.status_code!=200:\n",
    "        raise Exception('Failed to load page {}'.format(topic_url))\n",
    "    \n",
    "    #parse html\n",
    "    topic_doc=BeautifulSoup(response.text,'html.parser')\n",
    "\n",
    "    return topic_doc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_all_topic_repo_info(topic_doc):\n",
    "    #repo tags\n",
    "    usernames_and_repo_tag=topic_doc.find_all('h3',{'class':'f3 color-fg-muted text-normal lh-condensed'})\n",
    "\n",
    "    #stars count tags\n",
    "    star_tags=topic_doc.find_all('span',{'class':'Counter js-social-count'})\n",
    "\n",
    "    #using above tags to get repo info using function\n",
    "    topic_repo_dict={\n",
    "    'username':[],\n",
    "    'repo_name':[],\n",
    "    'repo_url':[],\n",
    "    'stars':[]\n",
    "    }\n",
    "\n",
    "    for i in range(len(usernames_and_repo_tag)):\n",
    "        repo_info=get_repo_info(usernames_and_repo_tag[i],star_tags[i])\n",
    "        topic_repo_dict['username'].append(repo_info[0])\n",
    "        topic_repo_dict['repo_name'].append(repo_info[1])\n",
    "        topic_repo_dict['repo_url'].append(repo_info[2])\n",
    "        topic_repo_dict['stars'].append(repo_info[3])\n",
    "    \n",
    "    return pd.DataFrame(topic_repo_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Below functions extract the exact data, ie. , 'topics_title' which is the name of the topic. 'topic_description' which is the description of the topic and 'topic_url', url of the topic page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to extract topic titles from title tags taken from the page\n",
    "def get_topics_title(doc):\n",
    "    selection_class='f3 lh-condensed mb-0 mt-1 Link--primary'\n",
    "    topic_names_p_tags=doc.find_all('p',{'class':selection_class})\n",
    "    topic_titles=[]\n",
    "    for tag in topic_names_p_tags:\n",
    "        topic_titles.append(tag.text)\n",
    "    \n",
    "    return topic_titles\n",
    "\n",
    "#function to extract description from the description tag\n",
    "#  extracted from the webpage\n",
    "def get_topic_description(doc):\n",
    "    topic_desc_class='f5 color-fg-muted mb-0 mt-1'\n",
    "    topic_description__tags=doc.find_all('p',{'class':topic_desc_class})\n",
    "    topic_descriptions=[]\n",
    "    for tag in topic_description__tags:\n",
    "        topic_descriptions.append(tag.text.strip())\n",
    "    \n",
    "    return topic_descriptions\n",
    "    \n",
    "#extract individual topic page link\n",
    "def get_topic_link(doc):\n",
    "    topic_link_tags=doc.find_all('a',{'class' : 'no-underline flex-grow-0'})\n",
    "    topic_urls=[]\n",
    "    base_url='https://github.com'\n",
    "    for tag in topic_link_tags:\n",
    "        topic_urls.append(base_url+tag['href'])\n",
    "    \n",
    "    return topic_urls \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using the above functions, below function will create a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that creates a csv file containing the list \n",
    "# of all topics present in the github/topics page\n",
    "def scrape_topics():\n",
    "    topics_url='https://github.com/topics'\n",
    "    response=requests.get(topics_url)\n",
    "    #check response\n",
    "    if response.status_code!=200:\n",
    "        raise Exception('Failed to load page {}'.format(topic_url))\n",
    "    \n",
    "    topics_dict={\n",
    "        'title':get_topics_title(doc),\n",
    "        'description':get_topic_description(doc),\n",
    "        'url':get_topic_link(doc)\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame(topics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dataframe is converted into a csv file and stored on a folder on the device using the os library, as per name extracted from above title extraction function 'get_topics_title()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv_of_each_topic(topic_url,topic_name):\n",
    "    topic_df=get_all_topic_repo_info(get_topic_page(topic_url))\n",
    "    file_name=topic_name+'.csv'\n",
    "\n",
    "    if os.path.exists(file_name):\n",
    "        print('The file \"{}\" already exists.'.format(file_name))\n",
    "        return\n",
    "\n",
    "    topic_df.to_csv(topic_name+'.csv',index=None)\n",
    "\n",
    "def scrape_repo_info_of_topic():\n",
    "    topics_df =scrape_topics()\n",
    "\n",
    "    os.makedirs('topics_data',exist_ok=True)\n",
    "\n",
    "    for index,row in topics_df.iterrows():\n",
    "        # print('scraping top repositories for \"{}\"'.format(row['title']))\n",
    "        print('scraping top repositories for \"{}\"'.format(row['title']))\n",
    "        create_csv_of_each_topic(row['url'],'topics_data/{}.csv'.format(row['title']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Call the final function, 'scrape_repo_info_of_topic()'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping top repositories for \"3D\"\n",
      "scraping top repositories for \"Ajax\"\n",
      "scraping top repositories for \"Algorithm\"\n",
      "scraping top repositories for \"Amp\"\n",
      "scraping top repositories for \"Android\"\n",
      "scraping top repositories for \"Angular\"\n",
      "scraping top repositories for \"Ansible\"\n",
      "scraping top repositories for \"API\"\n",
      "scraping top repositories for \"Arduino\"\n",
      "scraping top repositories for \"ASP.NET\"\n",
      "scraping top repositories for \"Awesome Lists\"\n",
      "scraping top repositories for \"Amazon Web Services\"\n",
      "scraping top repositories for \"Azure\"\n",
      "scraping top repositories for \"Babel\"\n",
      "scraping top repositories for \"Bash\"\n",
      "scraping top repositories for \"Bitcoin\"\n",
      "scraping top repositories for \"Bootstrap\"\n",
      "scraping top repositories for \"Bot\"\n",
      "scraping top repositories for \"C\"\n",
      "scraping top repositories for \"Chrome\"\n",
      "scraping top repositories for \"Chrome extension\"\n",
      "scraping top repositories for \"Command-line interface\"\n",
      "scraping top repositories for \"Clojure\"\n",
      "scraping top repositories for \"Code quality\"\n",
      "scraping top repositories for \"Code review\"\n",
      "scraping top repositories for \"Compiler\"\n",
      "scraping top repositories for \"Continuous integration\"\n",
      "scraping top repositories for \"C++\"\n",
      "scraping top repositories for \"Cryptocurrency\"\n",
      "scraping top repositories for \"Crystal\"\n"
     ]
    }
   ],
   "source": [
    "scrape_repo_info_of_topic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folder named 'topics_data' contains the final results, which is all csv files containing the data extracted from the topic page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
